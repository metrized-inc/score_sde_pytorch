{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yang-song/score_sde_pytorch/blob/main/Score_SDE_demo_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBJappt3toqj"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "1. `git clone https://github.com/yang-song/score_sde_pytorch.git`\n",
        "\n",
        "2. Install [required packages](https://github.com/yang-song/score_sde_pytorch/blob/main/requirements.txt)\n",
        "\n",
        "3. `cd` into folder `score_sde_pytorch`, launch a local jupyter server and connect to colab following [these instructions](https://research.google.com/colaboratory/local-runtimes.html)\n",
        "\n",
        "4. Download pre-trained [checkpoints](https://drive.google.com/drive/folders/1tFmF_uh57O6lx9ggtZT_5LdonVK2cV-e?usp=sharing) and save them in the `exp` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa9OIcJmUKmZ",
        "outputId": "8fde468b-2c95-4003-f0bd-20ddad5248e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tensorflow_gan\\python\\estimator\\tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Autoload all modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import importlib\n",
        "import os\n",
        "import functools\n",
        "import itertools\n",
        "import torch\n",
        "from losses import get_optimizer\n",
        "from models.ema import ExponentialMovingAverage\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_gan as tfgan\n",
        "import tqdm\n",
        "import io\n",
        "import likelihood\n",
        "import controllable_generation\n",
        "from utils import restore_checkpoint\n",
        "sns.set(font_scale=2)\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "import models\n",
        "from models import utils as mutils\n",
        "from models import ncsnv2\n",
        "from models import ncsnpp\n",
        "from models import ddpm as ddpm_model\n",
        "from models import layerspp\n",
        "from models import layers\n",
        "from models import normalization\n",
        "import sampling\n",
        "from likelihood import get_likelihood_fn\n",
        "from sde_lib import VESDE, VPSDE, subVPSDE\n",
        "from sampling import (ReverseDiffusionPredictor, \n",
        "                      LangevinCorrector, \n",
        "                      EulerMaruyamaPredictor, \n",
        "                      AncestralSamplingPredictor, \n",
        "                      NoneCorrector, \n",
        "                      NonePredictor,\n",
        "                      AnnealedLangevinDynamics)\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "-reedYgCU79v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YESSIR\n"
          ]
        }
      ],
      "source": [
        "# @title Load the score-based model\n",
        "sde = 'VESDE' #@param ['VESDE', 'VPSDE', 'subVPSDE'] {\"type\": \"string\"}\n",
        "if sde.lower() == 'vesde':\n",
        "  from configs.ve import cifar10_ncsnpp_continuous as configs\n",
        "\n",
        "  print('YESSIR')\n",
        "  ckpt_filename = \"exp/ve/cifar10_ncsnpp_continuous/checkpoint_24.pth\"\n",
        "  config = configs.get_config()  \n",
        "  sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
        "  sampling_eps = 1e-5\n",
        "elif sde.lower() == 'vpsde':\n",
        "  from configs.vp import cifar10_ddpmpp_continuous as configs  \n",
        "  ckpt_filename = \"exp/vp/cifar10_ddpmpp_continuous/checkpoint_8.pth\"\n",
        "  config = configs.get_config()\n",
        "  sde = VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
        "  sampling_eps = 1e-3\n",
        "elif sde.lower() == 'subvpsde':\n",
        "  from configs.subvp import cifar10_ddpmpp_continuous as configs\n",
        "  ckpt_filename = \"exp/subvp/cifar10_ddpmpp_continuous/checkpoint_26.pth\"\n",
        "  config = configs.get_config()\n",
        "  sde = subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
        "  sampling_eps = 1e-3\n",
        "\n",
        "batch_size = 32 #@param {\"type\":\"integer\"}\n",
        "config.training.batch_size = batch_size\n",
        "config.eval.batch_size = batch_size\n",
        "\n",
        "random_seed = 0 #@param {\"type\": \"integer\"}\n",
        "\n",
        "sigmas = mutils.get_sigmas(config)\n",
        "scaler = datasets.get_data_scaler(config)\n",
        "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
        "score_model = mutils.create_model(config)\n",
        "\n",
        "optimizer = get_optimizer(config, score_model.parameters())\n",
        "ema = ExponentialMovingAverage(score_model.parameters(),\n",
        "                               decay=config.model.ema_rate)\n",
        "state = dict(step=0, optimizer=optimizer,\n",
        "             model=score_model, ema=ema)\n",
        "\n",
        "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
        "ema.copy_to(score_model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "G8ei2Xsfg6JQ"
      },
      "outputs": [],
      "source": [
        "#@title Visualization code\n",
        "\n",
        "def image_grid(x):\n",
        "  size = config.data.image_size\n",
        "  channels = config.data.num_channels\n",
        "  img = x.reshape(-1, size, size, channels)\n",
        "  w = int(np.sqrt(img.shape[0]))\n",
        "  img = img.reshape((w, w, size, size, channels)).transpose((0, 2, 1, 3, 4)).reshape((w * size, w * size, channels))\n",
        "  return img\n",
        "\n",
        "def show_samples(x):\n",
        "  x = x.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "  img = image_grid(x)\n",
        "  plt.figure(figsize=(8,8))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hbBGjCMNUsp"
      },
      "source": [
        "# Predictor Corrector sampling\n",
        "\n",
        "\n",
        "Recommended settings:\n",
        "\n",
        " | dataset | SDE | predictor | corrector | snr | n_steps |\n",
        "|:----:|:----:|:----------:|:--------:|:---:|:----:|\n",
        "|CIFAR-10 | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.16| 1|\n",
        "|CIFAR-10 | VP | EulerMaruyamaPredictor | None | - | - |\n",
        "|CIFAR-10 | subVP| EulerMaruyamaPredictor | None | - | - |\n",
        "| LSUN/CelebA-HQ/FFHQ 256px | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.075 | 1 |\n",
        "| FFHQ 1024px | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.15| 1 |\n",
        "\n",
        "Check `probability_flow` to run PC sampling based on discretizing the probability flow ODE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "_X41BhiLqJvM",
        "outputId": "8a5b3b5f-93ad-4baf-d66f-0a648f935170"
      },
      "outputs": [],
      "source": [
        "#@title PC sampling\n",
        "img_size = config.data.image_size\n",
        "channels = config.data.num_channels\n",
        "shape = (batch_size, channels, img_size, img_size)\n",
        "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
        "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
        "snr = 0.16 #@param {\"type\": \"number\"}\n",
        "n_steps =  1#@param {\"type\": \"integer\"}\n",
        "probability_flow = False #@param {\"type\": \"boolean\"}\n",
        "sampling_fn = sampling.get_pc_sampler(sde, shape, predictor, corrector,\n",
        "                                      inverse_scaler, snr, n_steps=n_steps,\n",
        "                                      probability_flow=probability_flow,\n",
        "                                      continuous=config.training.continuous,\n",
        "                                      eps=sampling_eps, device=config.device)\n",
        "\n",
        "x, n = sampling_fn(score_model)\n",
        "show_samples(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AdiQdwN2aFA"
      },
      "source": [
        "# Probability flow ODE\n",
        "\n",
        "With black-box ODE solvers, we can produce samples, compute likelihoods, and obtain a uniquely identifiable encoding of any data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "iLQDfFvHSIGn",
        "outputId": "f1888e8b-4e70-446d-d248-9f1c1b6a7916"
      },
      "outputs": [],
      "source": [
        " #@title ODE sampling\n",
        "\n",
        "shape = (batch_size, 3, 32, 32)\n",
        "sampling_fn = sampling.get_ode_sampler(sde,                                        \n",
        "                                       shape, \n",
        "                                       inverse_scaler,                                       \n",
        "                                       denoise=True, \n",
        "                                       eps=sampling_eps,\n",
        "                                       device=config.device)\n",
        "x, nfe = sampling_fn(score_model)\n",
        "show_samples(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MsdcLnhu7s46"
      },
      "outputs": [],
      "source": [
        "#@title Likelihood computation\n",
        "train_ds, eval_ds, _ = datasets.get_dataset(config, uniform_dequantization=True, evaluation=True)\n",
        "eval_iter = iter(eval_ds)\n",
        "bpds = []\n",
        "likelihood_fn = likelihood.get_likelihood_fn(sde,                                              \n",
        "                                             inverse_scaler,                                             \n",
        "                                             eps=1e-5)\n",
        "for batch in eval_iter:\n",
        "  img = batch['image']._numpy()\n",
        "  img = torch.tensor(img).permute(0, 3, 1, 2).to(config.device)\n",
        "  img = scaler(img)\n",
        "  bpd, z, nfe = likelihood_fn(score_model, img)\n",
        "  bpds.extend(bpd)\n",
        "  print(f\"average bpd: {torch.tensor(bpds).mean().item()}, NFE: {nfe}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "oe3rLGRm28nc",
        "outputId": "4d3b614b-df5b-4523-aa91-a223d6134397"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
            "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
            "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
            "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 98304 into shape (5,5,32,32,3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32me:\\Metrized\\score_sde_pytorch\\Score_SDE_demo_PyTorch.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(image_grid(eval_images))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mOriginal images\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m eval_images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(eval_images)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
            "\u001b[1;32me:\\Metrized\\score_sde_pytorch\\Score_SDE_demo_PyTorch.ipynb Cell 11\u001b[0m in \u001b[0;36mimage_grid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m img \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, size, size, channels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m w \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqrt(img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mreshape((w, w, size, size, channels))\u001b[39m.\u001b[39mtranspose((\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m))\u001b[39m.\u001b[39mreshape((w \u001b[39m*\u001b[39m size, w \u001b[39m*\u001b[39m size, channels))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Metrized/score_sde_pytorch/Score_SDE_demo_PyTorch.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
            "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 98304 into shape (5,5,32,32,3)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Representations\n",
        "train_ds, eval_ds, _ = datasets.get_dataset(config, uniform_dequantization=False, evaluation=True)\n",
        "eval_batch = next(iter(eval_ds))\n",
        "eval_images = eval_batch['image']._numpy()\n",
        "shape = (batch_size, 3, 32, 32)\n",
        "\n",
        "likelihood_fn = likelihood.get_likelihood_fn(sde, inverse_scaler, eps=1e-5)\n",
        "sampling_fn = sampling.get_ode_sampler(sde, shape, inverse_scaler,\n",
        "                                       denoise=True, eps=sampling_eps, device=config.device)\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis('off')\n",
        "plt.imshow(image_grid(eval_images))\n",
        "plt.title('Original images')\n",
        "\n",
        "eval_images = torch.from_numpy(eval_images).permute(0, 3, 1, 2).to(config.device)\n",
        "_, latent_z, _ = likelihood_fn(score_model, scaler(eval_images))\n",
        "\n",
        "x, nfe = sampling_fn(score_model, latent_z)\n",
        "\n",
        "x = x.permute(0, 2, 3, 1).cpu().numpy()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis('off')\n",
        "plt.imshow(image_grid(x))\n",
        "plt.title('Reconstructed images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaGYVD7KcoW6"
      },
      "source": [
        "# Controllable generation\n",
        "\n",
        "Several demonstrations on how to solve inverse problems with our SDE framework.\n",
        "\n",
        "Recommended settings\n",
        "\n",
        "| dataset | SDE | predictor | corrector | snr | n_steps |\n",
        "|:----:|:----:|:----------:|:--------:|:---:|:----:|\n",
        "|CIFAR-10 | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.16| 1|\n",
        "|CIFAR-10 | VP | EulerMaruyamaPredictor | None | - | - |\n",
        "|CIFAR-10 | subVP| EulerMaruyamaPredictor | None | - | - |\n",
        "| LSUN/CelebA-HQ/FFHQ 256px | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.075 | 1 |\n",
        "| FFHQ 1024px | VE | ReverseDiffusionPredictor | LangevinCorrector | 0.15| 1 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tbly_8RIjqJD",
        "outputId": "28ca290e-1079-4031-e37a-c69374398f76"
      },
      "outputs": [],
      "source": [
        "#@title PC inpainting\n",
        "train_ds, eval_ds, _ = datasets.get_dataset(config)\n",
        "eval_iter = iter(eval_ds)\n",
        "bpds = []\n",
        "\n",
        "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
        "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
        "snr = 0.16 #@param {\"type\": \"number\"}\n",
        "n_steps = 1 #@param {\"type\": \"integer\"}\n",
        "probability_flow = False #@param {\"type\": \"boolean\"}\n",
        "\n",
        "pc_inpainter = controllable_generation.get_pc_inpainter(sde,\n",
        "                                                        predictor, corrector,\n",
        "                                                        inverse_scaler,\n",
        "                                                        snr=snr,\n",
        "                                                        n_steps=n_steps,\n",
        "                                                        probability_flow=probability_flow,\n",
        "                                                        continuous=config.training.continuous,\n",
        "                                                        denoise=True)\n",
        "batch = next(eval_iter)\n",
        "img = batch['image']._numpy()\n",
        "img = torch.from_numpy(img).permute(0, 3, 1, 2).to(config.device)\n",
        "show_samples(img)\n",
        "\n",
        "mask = torch.ones_like(img)\n",
        "mask[:, :, :, 16:] = 0.\n",
        "show_samples(img * mask)\n",
        "\n",
        "\n",
        "x = pc_inpainter(score_model, scaler(img), mask)\n",
        "show_samples(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DsP-ayb8cupk",
        "outputId": "51272ecd-4ba3-4931-8a6d-358c6218e25b"
      },
      "outputs": [],
      "source": [
        "#@title PC colorizer\n",
        "train_ds, eval_ds, _ = datasets.get_dataset(config)\n",
        "eval_iter = iter(eval_ds)\n",
        "bpds = []\n",
        "\n",
        "predictor = ReverseDiffusionPredictor #@param [\"EulerMaruyamaPredictor\", \"AncestralSamplingPredictor\", \"ReverseDiffusionPredictor\", \"None\"] {\"type\": \"raw\"}\n",
        "corrector = LangevinCorrector #@param [\"LangevinCorrector\", \"AnnealedLangevinDynamics\", \"None\"] {\"type\": \"raw\"}\n",
        "snr = 0.16 #@param {\"type\": \"number\"}\n",
        "n_steps = 1 #@param {\"type\": \"integer\"}\n",
        "probability_flow = False #@param {\"type\": \"boolean\"}\n",
        "\n",
        "batch = next(eval_iter)\n",
        "img = batch['image']._numpy()\n",
        "img = torch.from_numpy(img).permute(0, 3, 1, 2).to(config.device)\n",
        "show_samples(img)\n",
        "gray_scale_img = torch.mean(img, dim=1, keepdims=True).repeat(1, 3, 1, 1)\n",
        "show_samples(gray_scale_img)\n",
        "gray_scale_img = scaler(gray_scale_img)\n",
        "pc_colorizer = controllable_generation.get_pc_colorizer(\n",
        "    sde, predictor, corrector, inverse_scaler,\n",
        "    snr=snr, n_steps=n_steps, probability_flow=probability_flow,\n",
        "    continuous=config.training.continuous, denoise=True\n",
        ")\n",
        "x = pc_colorizer(score_model, gray_scale_img)\n",
        "\n",
        "show_samples(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiYRNB-Wk329"
      },
      "source": [
        "## Class-conditional generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTu-5e6S68Gb"
      },
      "source": [
        "Check out the [class-conditional generation section](https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55#scrollTo=HiYRNB-Wk329&line=3&uniqifier=1) in our [JAX demo](https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55?usp=sharing)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Score SDE demo PyTorch",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "739f515fbc693b0ef5b930780eef1a1b1825770e415178b662f8085df40d8af9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
